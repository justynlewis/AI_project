{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Summaries\n",
    "## Justyn Lewis and Emery Jacobowitz\n",
    "\n",
    "---\n",
    "\n",
    "Reading is hard. It would be really nice if we could just plug texts into a computer program, which could succinctly explain what's going on. This is the summarization problem. How do we figure out what information a text is trying to convey, and what to report in our summary.\n",
    "\n",
    "There are two approaches to summarization.\n",
    "\n",
    "The first is **extractive** summarization, where we come up with some way of ranking how important sentences are, then cutting out everything except those sentences. This is pretty easy to do, and it retains the original linguistic structure, but it's not how humans summarize things.\n",
    "\n",
    "The \"natural\" approach is **abstractive** summarization. This is where we read through a text, and come up with a novel summary that captures the main ideas. This is the approach we will be implementing today.\n",
    "\n",
    "We start by importing a dataset. This is a set of Amazon reviews for various projects over the span of a few years. Each one comes with a sample summary, which we will use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Getting the data\n",
    "# This is a dataset of 500k Amazon reviews\n",
    "# We only use 100k of these.\n",
    "data = pd.read_csv('data' + os.sep + 'Reviews.csv', nrows=1000)\n",
    "data.drop_duplicates(subset=['Text'],inplace=True)  #dropping duplicates\n",
    "data.dropna(axis=0,inplace=True)   #dropping na\n",
    "text_data = data['Text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "2.10.0\n",
      "False\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)\n",
    "\n",
    "print(tf.test.is_built_with_cuda())\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prologue: Preparing the data\n",
    "\n",
    "The first step when dealing with raw text is to clean it up, so we will use NLTK's NLP utilities to do this. This step includes:\n",
    "- Case normalization\n",
    "- Lemmatizing\n",
    "    - i.e. replacing inflected words with their \"dictionary forms\"\n",
    "- Removing punctuation\n",
    "- etc.\n",
    "\n",
    "To accomplish this, we will use a **tokenizer** from Keras. The goal is to train the tokenizer on our dataset, to create a \"vocabulary\" which accurately represents the kind of things we are going to summarize. The tokenizer also cleans the data, as above.\n",
    "\n",
    "Before the tokenizer runs, we first need to append all of the summaries with tags representing their starting and ending points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_START_ Good Quality Dog Food _END_',\n",
       " '_START_ Not as Advertised _END_',\n",
       " '_START_ \"Delight\" says it all _END_']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_data = data['Summary']\n",
    "summary_data = ['_START_ ' + summary + ' _END_' for summary in summary_data]\n",
    "\n",
    "summary_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# AKA out-of-vocabulary token\n",
    "# For unknown words that aren't relevant to tokenization\n",
    "oov = '_oov_'\n",
    "\n",
    "# initialize the tokenizer\n",
    "text_tokenizer = Tokenizer(oov_token=oov)\n",
    "\n",
    "# fit it on our dataset\n",
    "text_tokenizer.fit_on_texts(text_data)\n",
    "\n",
    "# split the text up into sequences\n",
    "input_seqs = text_tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "# repeat for the summaries\n",
    "summary_tokenizer = Tokenizer(oov_token=oov)\n",
    "summary_tokenizer.fit_on_texts(summary_data)\n",
    "target_seqs = text_tokenizer.texts_to_sequences(summary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vocabulary size:  6093\n",
      "Target vocabulary size:  1186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[43], [47], [659], [1], []]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Input vocabulary size: ', len(text_tokenizer.word_index))\n",
    "print('Target vocabulary size: ', len(summary_tokenizer.word_index))\n",
    "text_tokenizer.texts_to_sequences(['We', 'love', 'Artificial', 'Intelligence', '!'])\n",
    "# The output is a measure of how frequent these terms are in our dataset.\n",
    "# Notice that \"!\" is removed, because the tokenizer strips out punctuation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last bit of preprocessing is to pad and truncate our sequences, since we are going to want to always feed in inputs of the same length to our model. This is why we needed to label the starts and ends of the summaries. The maximum length after preprocessing will just be the average item length plus a buffer for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input maxlen:   419\n",
      "Target maxlen:  62\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from math import floor\n",
    "import numpy\n",
    "\n",
    "input_max_len = floor(numpy.average([len(item) for item in text_data])) + 25\n",
    "target_max_len = floor(numpy.average([len(item) for item in summary_data])) + 25\n",
    "\n",
    "print(\"Input maxlen:  \", input_max_len)\n",
    "print(\"Target maxlen: \", target_max_len)\n",
    "\n",
    "# the 'post' options mean we pad/truncate at the end of the sequence, not before it.\n",
    "input_seqs = pad_sequences(input_seqs, maxlen=input_max_len, padding='post', truncating='post')\n",
    "target_seqs = pad_sequences(target_seqs, maxlen=target_max_len, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final adjustment is to set the buffer and batch sizes for our model, producing a randomized Tensorflow Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None, 419), dtype=tf.int32, name=None), TensorSpec(shape=(None, 62), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER = 20000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "input_seqs = tf.cast(input_seqs, dtype=tf.int32)\n",
    "target_seqs = tf.cast(target_seqs, dtype=tf.int32)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_seqs, target_seqs)).shuffle(BUFFER).batch(BATCH_SIZE)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[ 31,  38,  17, ...,   0,   0,   0],\n",
       "          [304,  25, 559, ...,   0,   0,   0],\n",
       "          [454, 313, 202, ...,   0,   0,   0],\n",
       "          ...,\n",
       "          [  3, 141,   2, ...,   0,   0,   0],\n",
       "          [ 39,   7,  15, ...,   0,   0,   0],\n",
       "          [ 20,  18,  11, ...,   0,   0,   0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008,  148,    4, ...,    0,    0,    0],\n",
       "          [1008,  559,  304, ...,    0,    0,    0],\n",
       "          [1008,  436,  582, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008,  253,   34, ...,    0,    0,    0],\n",
       "          [1008,  226,   46, ...,    0,    0,    0],\n",
       "          [1008,   47,   11, ...,    0,    0,    0]])>)),\n",
       " (1,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[  94,  694,    6, ...,    0,    0,    0],\n",
       "          [   3,   18,  487, ...,    0,    0,    0],\n",
       "          [  15,  308,  371, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [  17,   23,   14, ...,    0,    0,    0],\n",
       "          [   8,  193, 5642, ...,    0,    0,    0],\n",
       "          [  43,   47,   63, ...,    0,    0,    0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008,   39,  649, ...,    0,    0,    0],\n",
       "          [1008,   46,   35, ...,    0,    0,    0],\n",
       "          [1008,   39,    7, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008,  794,  667, ...,    0,    0,    0],\n",
       "          [1008,    2,   61, ...,    0,    0,    0],\n",
       "          [1008,    2,   61, ...,    0,    0,    0]])>)),\n",
       " (2,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[ 117, 4767,    5, ...,    0,    0,    0],\n",
       "          [ 263,  111, 2256, ...,    0,    0,    0],\n",
       "          [  15,  308,  200, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [ 405,  431,    8, ...,    0,    0,    0],\n",
       "          [   2,   34,   25, ...,    0,    0,    0],\n",
       "          [ 265,  987,    4, ...,    0,    0,    0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008,   63,   23, ...,    0,    0,    0],\n",
       "          [1008,  111, 2256, ...,    0,    0,    0],\n",
       "          [1008,   35,  235, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008, 2526,  241, ...,    0,    0,    0],\n",
       "          [1008,  135,   78, ...,    0,    0,    0],\n",
       "          [1008, 1175,  582, ...,    0,    0,    0]])>)),\n",
       " (3,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[ 11,   9,  39, ...,   0,   0,   0],\n",
       "          [862, 553,  12, ...,   0,   0,   0],\n",
       "          [ 11,   9,   5, ...,   0,   0,   0],\n",
       "          ...,\n",
       "          [  3, 122,  17, ...,   0,   0,   0],\n",
       "          [158, 483, 206, ...,   0,   0,   0],\n",
       "          [101,   5, 304, ...,   0,   0,   0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008, 2817,   38, ...,    0,    0,    0],\n",
       "          [1008, 3148,    1, ...,    0,    0,    0],\n",
       "          [1008,   17,  784, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008,  169,  631, ...,    0,    0,    0],\n",
       "          [1008,  459, 2211, ...,    0,    0,    0],\n",
       "          [1008,   35,  304, ...,    0,    0,    0]])>)),\n",
       " (4,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[   2,   23,  338, ...,    0,    0,    0],\n",
       "          [ 730,   11,   44, ...,    0,    0,    0],\n",
       "          [   3,   53, 1173, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [ 259,    5,  438, ...,    0,    0,    0],\n",
       "          [   3,  441,   36, ...,    0,    0,    0],\n",
       "          [  11,    9,    2, ...,    0,    0,    0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008,  170,  582, ...,    0,    0,    0],\n",
       "          [1008,  476, 3981, ...,    0,    0,    0],\n",
       "          [1008,  143,   44, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008,  120,   19, ...,    0,    0,    0],\n",
       "          [1008,   24,   13, ...,    0,    0,    0],\n",
       "          [1008,  194,  212, ...,    0,    0,    0]])>)),\n",
       " (5,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[  47,   11,   19, ...,    0,    0,    0],\n",
       "          [  11,    9,    5, ...,    0,    0,    0],\n",
       "          [  15, 2841,  399, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   5, 4398,  353, ...,    0,    0,    0],\n",
       "          [   3,   47, 1504, ...,    0,    0,    0],\n",
       "          [ 170,  675,  374, ...,    0,    0,    0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008, 5005, 2012, ...,    0,    0,    0],\n",
       "          [1008,  662,  118, ...,    0,    0,    0],\n",
       "          [1008,  458,   83, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008,    5,  465, ...,    0,    0,    0],\n",
       "          [1008,   35,   44, ...,    0,    0,    0],\n",
       "          [1008,  179,   21, ...,    0,    0,    0]])>)),\n",
       " (6,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[   3,   47,  288, ...,    0,    0,    0],\n",
       "          [   3,  392,   11, ...,    0,    0,    0],\n",
       "          [ 299,    5,  107, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   2,  229, 1296, ...,    0,    0,    0],\n",
       "          [  41,   22,  166, ...,    0,    0,    0],\n",
       "          [   3,  141,    3, ...,    0,    0,    0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008,  177,   56, ...,    0,    0,    0],\n",
       "          [1008,    2,   61, ...,    0,    0,    0],\n",
       "          [1008,  666,  740, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008,  546,   13, ...,    0,    0,    0],\n",
       "          [1008,    2,   61, ...,    0,    0,    0],\n",
       "          [1008,   61, 1871, ...,    0,    0,    0]])>)),\n",
       " (7,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[  3, 260,  17, ...,   0,   0,   0],\n",
       "          [ 11,   9,  46, ...,   0,   0,   0],\n",
       "          [  5,  31, 302, ...,   0,   0,   0],\n",
       "          ...,\n",
       "          [ 50,  42,  80, ...,   0,   0,   0],\n",
       "          [ 43,  18, 418, ...,   0,   0,   0],\n",
       "          [ 17, 100,  23, ...,   0,   0,   0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008,   47,   17, ...,    0,    0,    0],\n",
       "          [1008,   35,   46, ...,    0,    0,    0],\n",
       "          [1008,   35,  315, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008,   50,   80, ...,    0,    0,    0],\n",
       "          [1008,   35,   46, ...,    0,    0,    0],\n",
       "          [1008,  143,  582, ...,    0,    0,    0]])>)),\n",
       " (8,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[  15,  308,   49, ...,    0,    0,    0],\n",
       "          [  99,    4,  155, ...,    0,    0,    0],\n",
       "          [2350,    9,  162, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [ 143,    3,  617, ...,    0,    0,    0],\n",
       "          [   3,   45,  122, ...,    0,    0,    0],\n",
       "          [   3,   47,    2, ...,    0,    0,    0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008,  210,   44, ...,    0,    0,    0],\n",
       "          [1008,   40,   31, ...,    0,    0,    0],\n",
       "          [1008,   35,  304, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008,  274, 1231, ...,    0,    0,    0],\n",
       "          [1008,   31,  212, ...,    0,    0,    0],\n",
       "          [1008,    3,   47, ...,    0,    0,    0]])>)),\n",
       " (9,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[  15,  500, 2993, ...,    0,    0,    0],\n",
       "          [   3, 2402,  130, ...,    0,    0,    0],\n",
       "          [   3,   53, 1082, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   3,  122,   17, ...,    0,    0,    0],\n",
       "          [  11,    9,    5, ...,    0,    0,    0],\n",
       "          [   3,  122,   11, ...,    0,    0,    0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008,   47,    2, ...,    0,    0,    0],\n",
       "          [1008,   35,   56, ...,    0,    0,    0],\n",
       "          [1008,   95,   24, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008,  794,  246, ...,    0,    0,    0],\n",
       "          [1008,  101,    5, ...,    0,    0,    0],\n",
       "          [1008,   34, 1920, ...,    0,    0,    0]])>)),\n",
       " (10,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[   2, 1798, 2244, ...,    0,    0,    0],\n",
       "          [2231,  101,   53, ...,    0,    0,    0],\n",
       "          [ 412,    9,   83, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [  43,   18, 1222, ...,    0,    0,    0],\n",
       "          [   3,  105,    6, ...,    0,    0,    0],\n",
       "          [  17,  318,   14, ...,    0,    0,    0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008,  197, 2656, ...,    0,    0,    0],\n",
       "          [1008,   35,  133, ...,    0,    0,    0],\n",
       "          [1008,  546,  582, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008,   88,   54, ...,    0,    0,    0],\n",
       "          [1008,  730,  582, ...,    0,    0,    0],\n",
       "          [1008,   62,   72, ...,    0,    0,    0]])>)),\n",
       " (11,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[   3,   25,  714, ...,    0,    0,    0],\n",
       "          [  11,    9,   83, ...,    0,    0,    0],\n",
       "          [  63, 4478,   14, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1108,   96,  170, ...,    0,    0,    0],\n",
       "          [  11,   25,    2, ...,    0,    0,    0],\n",
       "          [  11,    9,    2, ...,    0,    0,    0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008,  385,  150, ...,    0,    0,    0],\n",
       "          [1008,  120, 2577, ...,    0,    0,    0],\n",
       "          [1008,   47,   29, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008,  143,  101, ...,    0,    0,    0],\n",
       "          [1008,   61,  304, ...,    0,    0,    0],\n",
       "          [1008,   61, 1347, ...,    0,    0,    0]])>)),\n",
       " (12,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[   3,   18,  122, ...,    0,    0,    0],\n",
       "          [   3,   18,  105, ...,    0,    0,    0],\n",
       "          [   3,   69,  128, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [  12,    2, 3526, ...,    0,    0,    0],\n",
       "          [  11,  212,   67, ...,    0,    0,    0],\n",
       "          [  15,  308,  144, ...,    0,    0,    0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008,   35,   38, ...,    0,    0,    0],\n",
       "          [1008,   15,  121, ...,    0,    0,    0],\n",
       "          [1008,   87,    1, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008,   61, 1229, ...,    0,    0,    0],\n",
       "          [1008,   35,   13, ...,    0,    0,    0],\n",
       "          [1008,  465,  582, ...,    0,    0,    0]])>)),\n",
       " (13,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[  11,   44,  461, ...,    0,    0,    0],\n",
       "          [1062,   90,  757, ...,    0,    0,    0],\n",
       "          [ 322,  557, 3493, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   3,  371,  305, ...,    0,    0,    0],\n",
       "          [2804, 2805, 1740, ...,    0,    0,    0],\n",
       "          [  15,  390,  200, ...,    0,    0,    0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008,    1,   56, ...,    0,    0,    0],\n",
       "          [1008,   88,   81, ...,    0,    0,    0],\n",
       "          [1008,   35, 5327, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008,   61,  428, ...,    0,    0,    0],\n",
       "          [1008,   35,  195, ...,    0,    0,    0],\n",
       "          [1008,  941,    7, ...,    0,    0,    0]])>)),\n",
       " (14,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[   3,   18,  108, ...,    0,    0,    0],\n",
       "          [  78, 4835,  704, ...,    0,    0,    0],\n",
       "          [  11,    9,    5, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [  17,   14,   92, ...,    0,    0,    0],\n",
       "          [   3,  264,   26, ...,    0,    0,    0],\n",
       "          [  86,  109, 1020, ...,    0,    0,    0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008, 1891,  982, ...,    0,    0,    0],\n",
       "          [1008,   35,   23, ...,    0,    0,    0],\n",
       "          [1008, 2110,  927, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008,   92,  234, ...,    0,    0,    0],\n",
       "          [1008,  815,   35, ...,    0,    0,    0],\n",
       "          [1008,    1,   12, ...,    0,    0,    0]])>)),\n",
       " (15,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[  11,  195,    9, ...,    0,    0,    0],\n",
       "          [   3,   25,   67, ...,    0,    0,    0],\n",
       "          [  94,  439,  367, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [  94,   48, 5020, ...,    0,    0,    0],\n",
       "          [  30,  234,    3, ...,    0,    0,    0],\n",
       "          [  17,   23,   14, ...,    0,    0,    0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008, 1781,  353, ...,    0,    0,    0],\n",
       "          [1008,   47,   63, ...,    0,    0,    0],\n",
       "          [1008,   67,  177, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008,    2, 1835, ...,    0,    0,    0],\n",
       "          [1008,   61,   37, ...,    0,    0,    0],\n",
       "          [1008,    3,   47, ...,    0,    0,    0]])>)),\n",
       " (16,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[   3,  344,   47, ...,    0,    0,    0],\n",
       "          [  15, 1355,  284, ...,    0,    0,    0],\n",
       "          [   3,   18,  193, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   3,   18,  109, ...,    0,    0,    0],\n",
       "          [  15,  221,   49, ...,    0,    0,    0],\n",
       "          [  41,   22,   14, ...,    0,    0,    0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008,  718,   28, ...,    0,    0,    0],\n",
       "          [1008, 1110,    6, ...,    0,    0,    0],\n",
       "          [1008,    5,  143, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008, 4339, 1461, ...,    0,    0,    0],\n",
       "          [1008,   35,   13, ...,    0,    0,    0],\n",
       "          [1008, 1796, 4468, ...,    0,    0,    0]])>)),\n",
       " (17,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[  15,  711,    4, ...,    0,    0,    0],\n",
       "          [   2, 1153,   34, ...,    0,    0,    0],\n",
       "          [   2,  277,  564, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1351,  924,   65, ...,    0,    0,    0],\n",
       "          [  67,  230, 3098, ...,    0,    0,    0],\n",
       "          [   2,  124,   75, ...,    0,    0,    0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008,  118, 2269, ...,    0,    0,    0],\n",
       "          [1008,  197,  195, ...,    0,    0,    0],\n",
       "          [1008,   34, 1145, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008,  448, 4428, ...,    0,    0,    0],\n",
       "          [1008,   15,    1, ...,    0,    0,    0],\n",
       "          [1008,    1,   83, ...,    0,    0,    0]])>)),\n",
       " (18,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[ 151, 1725,    4, ...,    0,    0,    0],\n",
       "          [  35,   34,   35, ...,    0,    0,    0],\n",
       "          [ 153,    3,   25, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   3,  392,    2, ...,    0,    0,    0],\n",
       "          [   3,  135,   17, ...,    0,    0,    0],\n",
       "          [  15, 1816,  122, ...,    0,    0,    0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008,  430,  790, ...,    0,    0,    0],\n",
       "          [1008,  271,    1, ...,    0,    0,    0],\n",
       "          [1008, 1532,    4, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008,  904,  582, ...,    0,    0,    0],\n",
       "          [1008,  181,   13, ...,    0,    0,    0],\n",
       "          [1008,  465,   82, ...,    0,    0,    0]])>)),\n",
       " (19,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[  3, 124, 116, ...,   0,   0,   0],\n",
       "          [  3,  47,  11, ...,   0,   0,   0],\n",
       "          [ 17,  14,   2, ...,   0,   0,   0],\n",
       "          ...,\n",
       "          [  3,  47,   2, ...,   0,   0,   0],\n",
       "          [  3, 190,  17, ...,   0,   0,   0],\n",
       "          [ 15, 308, 200, ...,   0,   0,   0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008,   61,  196, ...,    0,    0,    0],\n",
       "          [1008,   31,  304, ...,    0,    0,    0],\n",
       "          [1008,  559,   23, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008,   82, 1677, ...,    0,    0,    0],\n",
       "          [1008,  330,   31, ...,    0,    0,    0],\n",
       "          [1008,  308,  200, ...,    0,    0,    0]])>)),\n",
       " (20,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[   8,  464,    5, ...,    0,    0,    0],\n",
       "          [  41,  108,   28, ...,    0,    0,    0],\n",
       "          [  44,  322, 2266, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   3,  106,   30, ...,    0,    0,    0],\n",
       "          [  11,   91,    7, ...,    0,    0,    0],\n",
       "          [ 117,    5, 4988, ...,    0,    0,    0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008,  250,   31, ...,    0,    0,    0],\n",
       "          [1008, 1185,   22, ...,    0,    0,    0],\n",
       "          [1008,   24,   26, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008,   15,  151, ...,    0,    0,    0],\n",
       "          [1008,  210,   82, ...,    0,    0,    0],\n",
       "          [1008,  121,  788, ...,    0,    0,    0]])>)),\n",
       " (21,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[749, 406,  15, ...,   0,   0,   0],\n",
       "          [  3,  25, 305, ...,   0,   0,   0],\n",
       "          [ 11,   9,   2, ...,   0,   0,   0],\n",
       "          ...,\n",
       "          [122,  17,  13, ...,   0,   0,   0],\n",
       "          [  3,  67,  47, ...,   0,   0,   0],\n",
       "          [ 44,   9, 148, ...,   0,   0,   0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008,   35,  120, ...,    0,    0,    0],\n",
       "          [1008,   15,  229, ...,    0,    0,    0],\n",
       "          [1008,    1,    2, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008,   35,   44, ...,    0,    0,    0],\n",
       "          [1008,   24,   40, ...,    0,    0,    0],\n",
       "          [1008,    1,    1, ...,    0,    0,    0]])>)),\n",
       " (22,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[  15,  306,   18, ...,    0,    0,    0],\n",
       "          [   3,   69,  128, ...,    0,    0,    0],\n",
       "          [  17,   14,  978, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   3,  264,   11, ...,    0,    0,    0],\n",
       "          [ 867, 4588,   17, ...,    0,    0,    0],\n",
       "          [   6,   36, 1018, ...,    0,    0,    0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008,   15,  306, ...,    0,    0,    0],\n",
       "          [1008,   40,  265, ...,    0,    0,    0],\n",
       "          [1008,    2, 2233, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008,    1,   34, ...,    0,    0,    0],\n",
       "          [1008,  125,  375, ...,    0,    0,    0],\n",
       "          [1008,    2, 1639, ...,    0,    0,    0]])>)),\n",
       " (23,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[   3,  106,    5, ...,    0,    0,    0],\n",
       "          [   3,   25, 2423, ...,    0,    0,    0],\n",
       "          [ 117,  181,   32, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [ 182,   35,    4, ...,    0,    0,    0],\n",
       "          [ 789,   56,    9, ...,    0,    0,    0],\n",
       "          [ 730,    3,   18, ...,    0,    0,    0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008,   35,  133, ...,    0,    0,    0],\n",
       "          [1008,  265, 1075, ...,    0,    0,    0],\n",
       "          [1008,   35,  302, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008,  182,   35, ...,    0,    0,    0],\n",
       "          [1008,  789,   56, ...,    0,    0,    0],\n",
       "          [1008,   63,  100, ...,    0,    0,    0]])>)),\n",
       " (24,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[ 43,  27,  17, ...,   0,   0,   0],\n",
       "          [  3, 592, 108, ...,   0,   0,   0],\n",
       "          [  3, 608, 127, ...,   0,   0,   0],\n",
       "          ...,\n",
       "          [ 34,  27,   8, ...,   0,   0,   0],\n",
       "          [  3,  47,  63, ...,   0,   0,   0],\n",
       "          [ 69, 650,  90, ...,   0,   0,   0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008,   61,  667, ...,    0,    0,    0],\n",
       "          [1008,  665, 2110, ...,    0,    0,    0],\n",
       "          [1008, 2579,  200, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008,  258,   24, ...,    0,    0,    0],\n",
       "          [1008, 2840,   79, ...,    0,    0,    0],\n",
       "          [1008, 1232,  582, ...,    0,    0,    0]])>)),\n",
       " (25,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[  86,  360,   57, ...,    0,    0,    0],\n",
       "          [ 165,   17,   13, ...,    0,    0,    0],\n",
       "          [   3, 2136,   79, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [  13,   65,  590, ...,    0,    0,    0],\n",
       "          [   3,  116,   17, ...,    0,    0,    0],\n",
       "          [ 461,   19,  352, ...,    0,    0,    0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008,  465,  582, ...,    0,    0,    0],\n",
       "          [1008,   17,   14, ...,    0,    0,    0],\n",
       "          [1008, 3189,   56, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008,   35,   23, ...,    0,    0,    0],\n",
       "          [1008,  546,  582, ...,    0,    0,    0],\n",
       "          [1008,  559,  212, ...,    0,    0,    0]])>)),\n",
       " (26,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[  39,    7,    2, ...,    0,    0,    0],\n",
       "          [  41,  421, 3394, ...,    0,    0,    0],\n",
       "          [ 802,  108,    6, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   3,   18,    5, ...,    0,    0,    0],\n",
       "          [  11,   44,  441, ...,    0,    0,    0],\n",
       "          [  11,   25,   67, ...,    0,    0,    0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008,    5, 1556, ...,    0,    0,    0],\n",
       "          [1008,  132,   72, ...,    0,    0,    0],\n",
       "          [1008,   24, 1875, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008,   31,   13, ...,    0,    0,    0],\n",
       "          [1008,   67,  565, ...,    0,    0,    0],\n",
       "          [1008, 4335,  113, ...,    0,    0,    0]])>)),\n",
       " (27,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[ 111, 1312,   49, ...,    0,    0,    0],\n",
       "          [ 600,   25,   35, ...,    0,    0,    0],\n",
       "          [   3,  122,   17, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   3,   18,    5, ...,    0,    0,    0],\n",
       "          [   3,  344,  110, ...,    0,    0,    0],\n",
       "          [  51, 2186, 1771, ...,    0,    0,    0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008,   15,  221, ...,    0,    0,    0],\n",
       "          [1008, 4200, 4201, ...,    0,    0,    0],\n",
       "          [1008,   31,   23, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008,   35,   46, ...,    0,    0,    0],\n",
       "          [1008,  269,  192, ...,    0,    0,    0],\n",
       "          [1008,  643,  267, ...,    0,    0,    0]])>)),\n",
       " (28,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[   3,   47,   17, ...,    0,    0,    0],\n",
       "          [  11,  501,  503, ...,    0,    0,    0],\n",
       "          [  17,   23,   59, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [   3,   25, 1197, ...,    0,    0,    0],\n",
       "          [   3,  617,  623, ...,    0,    0,    0],\n",
       "          [  17, 4334,   14, ...,    0,    0,    0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008,    3,   47, ...,    0,    0,    0],\n",
       "          [1008, 1500,  172, ...,    0,    0,    0],\n",
       "          [1008,  126, 1069, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008, 1750,  527, ...,    0,    0,    0],\n",
       "          [1008,   42,  198, ...,    0,    0,    0],\n",
       "          [1008,  650,    7, ...,    0,    0,    0]])>)),\n",
       " (29,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[  11,    9,   24, ...,    0,    0,    0],\n",
       "          [  43,  144,  118, ...,    0,    0,    0],\n",
       "          [  11,  639,   12, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [ 465,   22,   53, ...,    0,    0,    0],\n",
       "          [  15, 1586, 3489, ...,    0,    0,    0],\n",
       "          [  66,   56,   38, ...,    0,    0,    0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008,   24,  932, ...,    0,    0,    0],\n",
       "          [1008,  197,   13, ...,    0,    0,    0],\n",
       "          [1008,  395,  151, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008,  465,   22, ...,    0,    0,    0],\n",
       "          [1008,  197,   13, ...,    0,    0,    0],\n",
       "          [1008,   66,   56, ...,    0,    0,    0]])>)),\n",
       " (30,\n",
       "  (<tf.Tensor: shape=(32, 419), dtype=int32, numpy=\n",
       "   array([[312, 833, 312, ...,   0,   0,   0],\n",
       "          [ 35, 861,  32, ...,   0,   0,   0],\n",
       "          [ 43,  14, 118, ...,   0,   0,   0],\n",
       "          ...,\n",
       "          [  2,  81,  28, ...,   0,   0,   0],\n",
       "          [ 15, 389,   9, ...,   0,   0,   0],\n",
       "          [ 17,  23,  59, ...,   0,   0,   0]])>,\n",
       "   <tf.Tensor: shape=(32, 62), dtype=int32, numpy=\n",
       "   array([[1008,   61,  366, ...,    0,    0,    0],\n",
       "          [1008,   35,  861, ...,    0,    0,    0],\n",
       "          [1008,   31,   46, ...,    0,    0,    0],\n",
       "          ...,\n",
       "          [1008,   50, 1243, ...,    0,    0,    0],\n",
       "          [1008,  193,  148, ...,    0,    0,    0],\n",
       "          [1008,   17,   23, ...,    0,    0,    0]])>)),\n",
       " (31,\n",
       "  (<tf.Tensor: shape=(5, 419), dtype=int32, numpy=\n",
       "   array([[  11,  758,  451, ...,    0,    0,    0],\n",
       "          [   3,  405, 1405, ...,    0,    0,    0],\n",
       "          [   2,  353,  162, ...,    0,    0,    0],\n",
       "          [ 781,    2,  933, ...,    0,    0,    0],\n",
       "          [  11,   49,    6, ...,    0,    0,    0]])>,\n",
       "   <tf.Tensor: shape=(5, 62), dtype=int32, numpy=\n",
       "   array([[1008,   35,   44,  582,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0],\n",
       "          [1008, 1293,  504,  143,    4, 1849,  120,  582,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0],\n",
       "          [1008, 1017, 2502,   24, 1017, 1970,  582,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0],\n",
       "          [1008, 1275,  582,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0],\n",
       "          [1008,    2,   61,   56,  130,    1, 1600,  652,  582,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "              0,    0,    0,    0,    0,    0,    0]])>))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(batch, (inp, tar)) for (batch, (inp, tar)) in enumerate(dataset)]"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer Architecture, pt. I\n",
    "---\n",
    "### Data Utilities\n",
    "The thing about self-attention in transformers is we could end up losing ordinal information about our data. To solve this problem, we use \"positional encodings\", where we embed ordinal information within our word data. We want to implement these formulae:\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math is hard.\n",
    "# This section is closely adapted from https://medium.com/swlh/abstractive-text-summarization-using-transformers-3e774cc42453\n",
    "\n",
    "# calculate PE\n",
    "def positional_encoding(pos, d_model):\n",
    "    angle_rads = get_angles(\n",
    "        numpy.arange(pos)[:, numpy.newaxis],\n",
    "        numpy.arange(d_model)[numpy.newaxis, :],\n",
    "        d_model\n",
    "    )\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = numpy.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = numpy.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[numpy.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# generate the internal argument to the trig function\n",
    "def get_angles(position, i, d_model):\n",
    "    angle_rates = 1 / numpy.power(10000, (2 * (i // 2)) / numpy.float32(d_model))\n",
    "    return position * angle_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need two small masks.\n",
    "\n",
    "One is the padding mask, which ignores the padded portions of our sequences.\n",
    "\n",
    "The other is the lookahead mask, which ignores words that come after a given word. This is important because we want to make predictions based on the words we have seen already, since language is linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([], shape=(0, 62), dtype=int32)\n",
      "tf.Tensor([], shape=(0, 1, 1, 62), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[0. 0. 0. ... 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 1. 1. 1.]]]], shape=(4, 1, 1, 419), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[0. 0. 0. ... 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 1. 1. 1.]]]], shape=(5, 1, 1, 419), dtype=float32)\n",
      "==================================================\n",
      "tf.Tensor(\n",
      "[[0. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]], shape=(5, 5), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. 1. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]], shape=(4, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. 1. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 0.]], shape=(3, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. 1.]\n",
      " [0. 0.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def padding_mask(seq):\n",
    "    # we padded with 0s\n",
    "    remove_padding = tf.math.equal(seq, 0)\n",
    "    new_seq = tf.cast(remove_padding, tf.float32)\n",
    "    # add two new axes\n",
    "    return new_seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "# Once again, I don't really understand how this mask works. It's from the same source above.\n",
    "def lookahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "print(target_seqs[:0])\n",
    "print(padding_mask(target_seqs[:0]))\n",
    "print(padding_mask(input_seqs[:4]))\n",
    "print(padding_mask(input_seqs[:5]))\n",
    "print('==================================================')\n",
    "print(lookahead_mask(5))\n",
    "print(lookahead_mask(4))\n",
    "print(lookahead_mask(3))\n",
    "print(lookahead_mask(2))"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer Architecture, pt. 2\n",
    "---\n",
    "### Paying Self-Attention\n",
    "\n",
    "At this point, we can start assembling the transformer. The first step is to implement a scaled dot product function, which is the key to our self-attention layer. This includes a few components: \n",
    "- *Matrix multiplication* (i.e. dot product) of our query and our key\n",
    "- *Scaling*\n",
    "    - We need to scale down our inputs in order to prevent them from being too large, so we divide by $\\sqrt{d_k}$, a scaling factor proportional to the size of our input.\n",
    "- An optional *mask*\n",
    "    - This is where we will use our padding mask from before\n",
    "- *Softmax*\n",
    "    - Transforming the value to fit in a \\[0, 1\\] probability distribution.\n",
    "- Another step of matrix multiplication of the result of the above with our value.\n",
    "\n",
    "Thus, the scaled dot product attention is equivalent to ![image.png](attachment:image.png) although the affect of the mask is not counted here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in a query, a key, a value, and an optional mask.\n",
    "# returns a tuple of (the output, the attention weights)\n",
    "def sdp_attention(q, k, v, mask=None):\n",
    "    # Step 1: QK^T, AKA matrix multiplication\n",
    "    qk = tf.matmul(q, k, transpose_b=True)\n",
    "    \n",
    "    #Scaling\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled = qk / tf.math.sqrt(dk)\n",
    "    \n",
    "    # apply the mask, only if it is not the default None option.\n",
    "    if mask is not None:\n",
    "        print('scaled: ', scaled)\n",
    "        print('mask:   ', mask)\n",
    "        scaled += (mask * -1e9)\n",
    "    \n",
    "    # softmax\n",
    "    softmax = tf.nn.softmax(scaled, axis=-1)\n",
    "    \n",
    "    # finally, multiply by v\n",
    "    result = tf.matmul(softmax, v)\n",
    "    return result, softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to actually make this into a layer in our Transformer architecture. We do this by implementing a subclass of the Layer class from Keras. The important method that we implement is call(), which applies the logic of the layer. Keras documentation also suggests implementing a few other methods, but we don't think those are necessary for our project, so we are going to tactically ignore them.\n",
    "\n",
    "We make it \"multi-headed\", which means that we can split up the calculations to make them more efficient. If we want to split into *h* heads, then the sizes of the query, key, and value (AKA the *depth*) will be equal to *d_model* / *h*. Therefore, the former must be evenly divisible by the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.AttentionLayer at 0x13818fab040>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, heads):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        \n",
    "        # d_model needs to be divisible by the number of heads\n",
    "        if not d_model % heads == 0:\n",
    "            raise RuntimeError('d_model (' + str(d_model) + ') must be divisible by heads (' + str(heads) + ').')\n",
    "        \n",
    "        self.depth = d_model // heads\n",
    "        \n",
    "        # make Keras densely-connected NN layers for the data we eventually have\n",
    "        self.q_layer = tf.keras.layers.Dense(d_model)\n",
    "        self.k_layer = tf.keras.layers.Dense(d_model)\n",
    "        self.v_layer = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        # dense layer that will eventually store the output\n",
    "        self.out_layer = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    def call(self, q, v, k, mask):       \n",
    "        # start applying all those layers we made\n",
    "        Q = self.q_layer(q)\n",
    "        V = self.q_layer(v)\n",
    "        K = self.q_layer(k)\n",
    "        \n",
    "        # split the heads apart, so that we are multi-headed\n",
    "        split_Q = self.split(Q, BATCH_SIZE)\n",
    "        split_V = self.split(V, BATCH_SIZE)\n",
    "        split_K = self.split(K, BATCH_SIZE)\n",
    "    \n",
    "        # apply the attention\n",
    "        print(\"HIIII\")\n",
    "        attention = sdp_attention(split_Q, split_K, split_V, mask)\n",
    "        weights = attention[1]\n",
    "        attention_results = tf.transpose(attention[0], perm=[0,2,1,3])\n",
    "        \n",
    "        # concatenate attention back to one vector\n",
    "        attention_results = tf.reshape(attention_results, (tf.shape(q)[0], -1, self.d_model))\n",
    "        output = self.out_layer(attention_results)\n",
    "        \n",
    "        return output, weights\n",
    "        \n",
    "    \n",
    "    # helper method for splitting the heads\n",
    "    def split(self, layer, batch):\n",
    "        Layer = tf.reshape(layer, (batch, -1, self.heads, self.depth))\n",
    "        Layer = tf.transpose(Layer, perm=[0,2,1,3])\n",
    "        return Layer\n",
    "        \n",
    "AttentionLayer(6, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feeding it Forward\n",
    "\n",
    "The last basic unit of a transformer that we have to implement is a Feed-Forward Network (FFN) layer. This is pretty simple, since we just need to sequentially go from one layer to another, and Keras has this functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffn(d_model, d_ffn):\n",
    "    l1 = tf.keras.layers.Dense(d_ffn)\n",
    "    l2 = tf.keras.layers.Dense(d_model)\n",
    "    return tf.keras.Sequential([l1, l2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer Architecture, pt. 3\n",
    "---\n",
    "### Building the Transformer \n",
    "\n",
    "So far, we've made the individual building blocks of the Transformer, which process data at each step. Now, we can build the main components: the Encoder and the Decoder. These are comprised of individual layers, each of which contain:\n",
    "- An attention layer\n",
    "- A FFN layer\n",
    "- A normalization layer\n",
    "- A dropout layer\n",
    "    - When training, randomly sets some inputs to 0, and distributes that value to the other inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, heads, d_ffn):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        # the MH Attention layer\n",
    "        self.att = AttentionLayer(d_model, heads)\n",
    "        \n",
    "        # the FFN  layer\n",
    "        self.ffn = ffn(d_model, d_ffn)\n",
    "        \n",
    "        # Normie layers\n",
    "        # we use a smaller epsilon value than standard\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=0.000001)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=0.000001)\n",
    "        \n",
    "        # Dropout layers\n",
    "        # We apply dropout 10% of the time.\n",
    "        self.drop1 = tf.keras.layers.Dropout(0.1)\n",
    "        self.drop2 = tf.keras.layers.Dropout(0.1)\n",
    "        \n",
    "    def call(self, datum, is_training, mask):\n",
    "        # Sorry for the weird formatting.\n",
    "        # I think this is the best way to present the process\n",
    "        \n",
    "        attention = self.att(datum, datum, datum, mask)[0]\n",
    "        attention = self.drop1(attention, training=is_training)\n",
    "        attention = self.norm1(datum + attention)\n",
    "        \n",
    "        # <...>   <---   normalize <--  dropout    <--   attention <-- input\n",
    "#         attention = self.norm1( datum + self.drop1( self.att(datum, datum, datum, mask), training=is_training))\n",
    "        \n",
    "        #           normalize     <--       dropout <-- ffn <-- <...>\n",
    "        feed = self.ffn(attention)\n",
    "        feed = self.drop2(feed, training=is_training)\n",
    "        feed = self.norm2(attention + feed)\n",
    "        \n",
    "#         feed = self.norm2( attention + self.drop2( self.ffn(attention), training=is_training))\n",
    "        \n",
    "        return feed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Decoder layer is pretty much the same as an Encoder layer, except that it has that extra attention layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, heads, d_ffn):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        # the MH Attention layers\n",
    "        self.att1 = AttentionLayer(d_model, heads)\n",
    "        self.att2 = AttentionLayer(d_model, heads)\n",
    "        \n",
    "        # the FFN  layer\n",
    "        self.ffn = ffn(d_model, d_ffn)\n",
    "        \n",
    "        # Normie layers\n",
    "        # we use a smaller epsilon value than standard\n",
    "        norm1 = tf.keras.layers.LayerNormalization(epsilon=0.000001)\n",
    "        norm2 = tf.keras.layers.LayerNormalization(epsilon=0.000001)\n",
    "        norm3 = tf.keras.layers.LayerNormalization(epsilon=0.000001)\n",
    "        \n",
    "        # Dropout layers\n",
    "        # We apply dropout 10% of the time.\n",
    "        drop1 = tf.keras.layers.Dropout(0.1)\n",
    "        drop2 = tf.keras.layers.Dropout(0.1)\n",
    "        drop3 = tf.keras.layers.Dropout(0.1)\n",
    "        \n",
    "    # The decoder actually needs to use the masks we defined earlier\n",
    "    def call(self, datum, encoder_result, is_training, look_mask, pad_mask):\n",
    "        \n",
    "        # The first attention layer\n",
    "        att1_res = self.att1(datum, datum, datum, look_mask)\n",
    "        att1_out = att1_res[0]\n",
    "        att1_weights = att1_res[1]\n",
    "        att1_out = self.norm1(datum + (self.drop1(att1_out, training=is_training)))\n",
    "        \n",
    "        # This extra attention layer takes in the input from the encoder as a query\n",
    "        # and combines it with the first layer's output, using. \n",
    "        att2_res = self.att2(out1, encoder_result, encoder_result, pad_mask)\n",
    "        att2_out = att2_res[0]\n",
    "        att2_weights = att1_res[1]\n",
    "        att2_out = self.norm2(att1_out + (self.drop2(att2_out, training=is_training)))\n",
    "        \n",
    "        # Finally, the ffn layer\n",
    "        ffn_out = self.norm3(att2_out + self.drop3(self.ffn(att2_out), training=is_training))\n",
    "        \n",
    "        return ffn_out, att1_weights, att2_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling the components\n",
    "\n",
    "Now that we have our Encoder/Decoder layers, we can actually put them together to form the Encoder and Decoder themselves.\n",
    "\n",
    "These are basically just bigger Keras Layers that have multiple smaller layers inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, layers, d_model, heads, d_ffn, vocab_size, max_pos):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.layers = layers\n",
    "\n",
    "        # creates an embedding layer with the size of the vocabulary\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, self.d_model)\n",
    "        \n",
    "        self.pos_enc = positional_encoding(max_pos, self.d_model)\n",
    "\n",
    "        self.encoder_layers = []\n",
    "        for i in range(layers):\n",
    "            l = EncoderLayer(d_model, heads, d_ffn)\n",
    "            self.encoder_layers.append(l)\n",
    "\n",
    "        self.drop = tf.keras.layers.Dropout(0.1)\n",
    "    \n",
    "    def call(self, datum, is_training, mask):\n",
    "        # get the length of the input\n",
    "        length = tf.shape(datum)[1]\n",
    "        \n",
    "        # get the input embedding\n",
    "        Datum = self.embedding(datum)\n",
    "        Datum = Datum * tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        \n",
    "        # apply the positional encoding\n",
    "        Datum += self.pos_enc[:, :length, :]\n",
    "        \n",
    "        # apply dropout\n",
    "        Datum = self.drop(Datum, training=is_training)\n",
    "        \n",
    "        # apply all of the Encoder layers in order\n",
    "        for layer in self.encoder_layers:\n",
    "            Datum = layer(Datum, is_training, mask)\n",
    "        \n",
    "        return Datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, layers, d_model, heads, d_ffn, vocab_size, max_pos):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.layers = layers\n",
    "\n",
    "        # creates an embedding layer with the size of the vocabulary\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, self.d_model)\n",
    "        \n",
    "        self.pos_enc = positional_encoding(max_pos, self.d_model)\n",
    "\n",
    "        self.decoder_layers = []\n",
    "        for i in range(layers):\n",
    "            l = DecoderLayer(d_model, heads, d_ffn)\n",
    "            self.decoder_layers.append(l)\n",
    "\n",
    "        self.drop = tf.keras.layers.Dropout(0.1)\n",
    "    \n",
    "    def call(self, datum, decoded_output, is_training, look_mask, pad_mask):\n",
    "        # get the length of the input\n",
    "        length = tf.shape(datum)[1]\n",
    "        attn_weights = {}\n",
    "        \n",
    "        # get the input embedding\n",
    "        Datum = self.embedding(datum)\n",
    "        Datum = Datum * tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        \n",
    "        # apply the positional encoding\n",
    "        Datum += self.pos_enc[:, :length, :]\n",
    "        \n",
    "        # apply dropout\n",
    "        Datum = self.drop(Datum, training=is_training)\n",
    "        \n",
    "        # apply all of the Decoder layers in order\n",
    "        for index, layer in enumerate(self.decoder_layers):\n",
    "            Datum, block1, block2 = layer(Datum, decoded_output, is_training, look_mask, pad_mask)\n",
    "            \n",
    "            # store the attention weights in a dictionary\n",
    "            attn_weights['decoder_layer{}_block1'.format(index+1)] = block1\n",
    "            attn_weights['decoder_layer{}_block2'.format(index+1)] = block2\n",
    "\n",
    "        \n",
    "        return Datum, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer Architecture, pt. 4\n",
    "---\n",
    "### The Transformer Itself\n",
    "\n",
    "Finally, we have all the pieces ready to put the Transformer together. To do this, we use the Model class from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, layers, d_model, heads, d_ffn, vocab_size_input, vocab_size_target, max_pos_input, max_pos_target):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        # the encoder\n",
    "        self.encoder = Encoder(layers, d_model, heads, d_ffn, vocab_size_input, max_pos_input)\n",
    "        \n",
    "        # the decoder\n",
    "        self.decoder = Decoder(layers, d_model, heads, d_ffn, vocab_size_target, max_pos_target)\n",
    "        \n",
    "        # a final transformation layer\n",
    "        self.output_layer = tf.keras.layers.Dense(vocab_size_target)\n",
    "        \n",
    "    def call(self, input_data, target_data, pad_mask_enc, pad_mask_dec, look_mask_dec, is_training):\n",
    "        \n",
    "        enc = self.encoder(input_data, is_training, pad_mask_enc)\n",
    "        print(\"encoding complete\")\n",
    "        \n",
    "        dec_results = self.decoder(target_data, enc, is_training, look_mask_dec, pad_mask_dec)\n",
    "        print(\"decoding complete\")\n",
    "        \n",
    "        result = self.output_layer(dec_results[0])\n",
    "        \n",
    "        return result, dec_results[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps: Training and Inference\n",
    "---\n",
    "Now that we have assembled our transformer model, we have to train it on the dataset. First, we actually create a Transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Transformer at 0x13817872050>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these parameters can be tweaked\n",
    "LAYERS  = 4\n",
    "D_MODEL = 64\n",
    "D_FFN   = 256\n",
    "HEADS   = 8\n",
    "EPOCHS  = 10\n",
    "\n",
    "enc_size = len(text_tokenizer.word_index) + 1\n",
    "dec_size = len(summary_tokenizer.word_index) + 1\n",
    "\n",
    "\n",
    "t = Transformer(LAYERS, D_MODEL, HEADS, D_FFN, enc_size, dec_size, enc_size, dec_size)\n",
    "t"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial we followed describes a training schedule which comes from the original Transformer paper:\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "We want to implement this to make out training more efficient. To implement this, we inherit from the LearningRateSchedule class from Keras, which means that we have to implement its call() method. This method takes in *step_num*, and produces *lrate*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.008838835>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Schedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    # warmup = the number of warmup steps\n",
    "    def __init__(self, d_model, warmup=100):\n",
    "        super(Schedule, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        \n",
    "        self.warmup = warmup\n",
    "        \n",
    "    def call(self, step_num):\n",
    "        step = tf.cast(step_num, tf.float32)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(tf.math.rsqrt(step), step * (self.warmup ** -1.5))\n",
    "\n",
    "Schedule(128).call(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the tutorial we were following stopped explaining training, except for some very broad statements. So, we read through their sample code, and tried to understand what was going on, step by step.\n",
    "\n",
    "First, we use our *lrate* schedule with a Keras Adam optimizer, using parameters suggested by the Transformer paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_optimizer = tf.keras.optimizers.Adam(Schedule(D_MODEL), beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a SparseCategoricalCrossEntropy object. This allows us to keep track of the amount of information lost between each of our predictions, and each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "# create a simple mean metric to evaluate loss\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "def loss(actual, predicted):\n",
    "    Loss = entropy(actual, predicted)\n",
    "    \n",
    "    # create and apply an inversion mask\n",
    "    # this is necessary because we use 0s to pad short sequences,\n",
    "    # and we don't want those to just have huge loss penalties.\n",
    "    mask = tf.cast(tf.math.logical_not(tf.math.equal(actual, 0), dtype=Loss.dtype))\n",
    "    Loss = Loss * mask\n",
    "    \n",
    "    # proportionalize the result\n",
    "    result = tf.reduce_sum(Loss) / tf.resuce_sum(mask)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll make a helper function to create the padding and lookahead masks, since those are annoying to do manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask maker, mask maker, make me a mask \n",
    "# returns a tuple of the encoding pad mask, the decoding pad mask,\n",
    "# and the decoding pad + lookahead mask.\n",
    "def mask_maker(input_, target_):\n",
    "    enc_pad_in = padding_mask(input_)\n",
    "    dec_pad_in = padding_mask(input_)\n",
    "    \n",
    "    dec_look = lookahead_mask(tf.shape(target_)[1])\n",
    "    dec_pad_targ = padding_mask(target_)\n",
    "    \n",
    "    # (enc_pad, dec_pad, lookahead)\n",
    "    return enc_pad_in, dec_pad_in, tf.maximum(dec_look, dec_pad_targ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a checkpoint manager, which saves the training states every so often.\n",
    "This snippet of utility code was taken directly from the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoints\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=t, optimizer=adam_optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can get onto the actual training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaled:  Tensor(\"transformer_4/encoder_4/encoder_layer_16/attention_layer_51/truediv:0\", shape=(32, 8, 419, 419), dtype=float32)\n",
      "mask:    Tensor(\"strided_slice_2:0\", shape=(32, 1, 1, 419), dtype=float32)\n",
      "scaled:  Tensor(\"transformer_4/encoder_4/encoder_layer_17/attention_layer_52/truediv:0\", shape=(32, 8, 419, 419), dtype=float32)\n",
      "mask:    Tensor(\"strided_slice_2:0\", shape=(32, 1, 1, 419), dtype=float32)\n",
      "scaled:  Tensor(\"transformer_4/encoder_4/encoder_layer_18/attention_layer_53/truediv:0\", shape=(32, 8, 419, 419), dtype=float32)\n",
      "mask:    Tensor(\"strided_slice_2:0\", shape=(32, 1, 1, 419), dtype=float32)\n",
      "scaled:  Tensor(\"transformer_4/encoder_4/encoder_layer_19/attention_layer_54/truediv:0\", shape=(32, 8, 419, 419), dtype=float32)\n",
      "mask:    Tensor(\"strided_slice_2:0\", shape=(32, 1, 1, 419), dtype=float32)\n",
      "scaled:  Tensor(\"transformer_4/decoder_4/decoder_layer_16/attention_layer_55/truediv:0\", shape=(32, 8, 61, 61), dtype=float32)\n",
      "mask:    Tensor(\"Maximum:0\", shape=(32, 1, 62, 62), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\emery\\AppData\\Local\\Temp\\ipykernel_13336\\3524479449.py\", line 12, in train  *\n        preds = t(\n    File \"C:\\Users\\emery\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_file6x4cvn8o.py\", line 11, in tf__call\n        dec_results = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(target_data), ag__.ld(enc), ag__.ld(is_training), ag__.ld(look_mask_dec), ag__.ld(pad_mask_dec)), None, fscope)\n    File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filemrimg9u6.py\", line 35, in tf__call\n        ag__.for_stmt(ag__.converted_call(ag__.ld(enumerate), (ag__.ld(self).decoder_layers,), None, fscope), None, loop_body, get_state, set_state, ('Datum',), {'iterate_names': '(index, layer)'})\n    File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filemrimg9u6.py\", line 28, in loop_body\n        (Datum, block1, block2) = ag__.converted_call(ag__.ld(layer), (ag__.ld(Datum), ag__.ld(decoded_output), ag__.ld(is_training), ag__.ld(look_mask), ag__.ld(pad_mask)), None, fscope)\n    File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filen9wsstew.py\", line 10, in tf__call\n        att1_res = ag__.converted_call(ag__.ld(self).att1, (ag__.ld(datum), ag__.ld(datum), ag__.ld(datum), ag__.ld(look_mask)), None, fscope)\n    File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filej9v05bqs.py\", line 16, in tf__call\n        attention = ag__.converted_call(ag__.ld(sdp_attention), (ag__.ld(split_Q), ag__.ld(split_K), ag__.ld(split_V), ag__.ld(mask)), None, fscope)\n    File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filecgcjf1ai.py\", line 31, in tf__sdp_attention\n        ag__.if_stmt(ag__.ld(mask) is not None, if_body, else_body, get_state, set_state, ('scaled',), 1)\n    File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filecgcjf1ai.py\", line 26, in if_body\n        scaled += mask * -1000000000.0\n\n    ValueError: Exception encountered when calling layer \"transformer_4\" \"                 f\"(type Transformer).\n    \n    in user code:\n    \n        File \"C:\\Users\\emery\\AppData\\Local\\Temp\\ipykernel_13336\\654210736.py\", line 18, in call  *\n            dec_results = self.decoder(target_data, enc, is_training, look_mask_dec, pad_mask_dec)\n        File \"C:\\Users\\emery\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filemrimg9u6.py\", line 35, in tf__call\n            ag__.for_stmt(ag__.converted_call(ag__.ld(enumerate), (ag__.ld(self).decoder_layers,), None, fscope), None, loop_body, get_state, set_state, ('Datum',), {'iterate_names': '(index, layer)'})\n        File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filemrimg9u6.py\", line 28, in loop_body\n            (Datum, block1, block2) = ag__.converted_call(ag__.ld(layer), (ag__.ld(Datum), ag__.ld(decoded_output), ag__.ld(is_training), ag__.ld(look_mask), ag__.ld(pad_mask)), None, fscope)\n        File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filen9wsstew.py\", line 10, in tf__call\n            att1_res = ag__.converted_call(ag__.ld(self).att1, (ag__.ld(datum), ag__.ld(datum), ag__.ld(datum), ag__.ld(look_mask)), None, fscope)\n        File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filej9v05bqs.py\", line 16, in tf__call\n            attention = ag__.converted_call(ag__.ld(sdp_attention), (ag__.ld(split_Q), ag__.ld(split_K), ag__.ld(split_V), ag__.ld(mask)), None, fscope)\n        File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filecgcjf1ai.py\", line 31, in tf__sdp_attention\n            ag__.if_stmt(ag__.ld(mask) is not None, if_body, else_body, get_state, set_state, ('scaled',), 1)\n        File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filecgcjf1ai.py\", line 26, in if_body\n            scaled += mask * -1000000000.0\n    \n        ValueError: Exception encountered when calling layer \"decoder_4\" \"                 f\"(type Decoder).\n        \n        in user code:\n        \n            File \"C:\\Users\\emery\\AppData\\Local\\Temp\\ipykernel_13336\\2448223447.py\", line 37, in call  *\n                Datum, block1, block2 = layer(Datum, decoded_output, is_training, look_mask, pad_mask)\n            File \"C:\\Users\\emery\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filen9wsstew.py\", line 10, in tf__call\n                att1_res = ag__.converted_call(ag__.ld(self).att1, (ag__.ld(datum), ag__.ld(datum), ag__.ld(datum), ag__.ld(look_mask)), None, fscope)\n            File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filej9v05bqs.py\", line 16, in tf__call\n                attention = ag__.converted_call(ag__.ld(sdp_attention), (ag__.ld(split_Q), ag__.ld(split_K), ag__.ld(split_V), ag__.ld(mask)), None, fscope)\n            File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filecgcjf1ai.py\", line 31, in tf__sdp_attention\n                ag__.if_stmt(ag__.ld(mask) is not None, if_body, else_body, get_state, set_state, ('scaled',), 1)\n            File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filecgcjf1ai.py\", line 26, in if_body\n                scaled += mask * -1000000000.0\n        \n            ValueError: Exception encountered when calling layer \"decoder_layer_16\" \"                 f\"(type DecoderLayer).\n            \n            in user code:\n            \n                File \"C:\\Users\\emery\\AppData\\Local\\Temp\\ipykernel_13336\\3294731785.py\", line 28, in call  *\n                    att1_res = self.att1(datum, datum, datum, look_mask)\n                File \"C:\\Users\\emery\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n                File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filej9v05bqs.py\", line 16, in tf__call\n                    attention = ag__.converted_call(ag__.ld(sdp_attention), (ag__.ld(split_Q), ag__.ld(split_K), ag__.ld(split_V), ag__.ld(mask)), None, fscope)\n                File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filecgcjf1ai.py\", line 31, in tf__sdp_attention\n                    ag__.if_stmt(ag__.ld(mask) is not None, if_body, else_body, get_state, set_state, ('scaled',), 1)\n                File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filecgcjf1ai.py\", line 26, in if_body\n                    scaled += mask * -1000000000.0\n            \n                ValueError: Exception encountered when calling layer \"attention_layer_55\" \"                 f\"(type AttentionLayer).\n                \n                in user code:\n                \n                    File \"C:\\Users\\emery\\AppData\\Local\\Temp\\ipykernel_13336\\32935471.py\", line 33, in call  *\n                        attention = sdp_attention(split_Q, split_K, split_V, mask)\n                    File \"C:\\Users\\emery\\AppData\\Local\\Temp\\ipykernel_13336\\3890593852.py\", line 15, in sdp_attention  *\n                        scaled += (mask * -1e9)\n                \n                    ValueError: Dimensions must be equal, but are 61 and 62 for '{{node transformer_4/decoder_4/decoder_layer_16/attention_layer_55/add}} = AddV2[T=DT_FLOAT](transformer_4/decoder_4/decoder_layer_16/attention_layer_55/truediv, transformer_4/decoder_4/decoder_layer_16/attention_layer_55/mul)' with input shapes: [32,8,61,61], [32,1,62,62].\n                \n                \n                Call arguments received by layer \"attention_layer_55\" \"                 f\"(type AttentionLayer):\n                   q=tf.Tensor(shape=(32, 61, 64), dtype=float32)\n                   v=tf.Tensor(shape=(32, 61, 64), dtype=float32)\n                   k=tf.Tensor(shape=(32, 61, 64), dtype=float32)\n                   mask=tf.Tensor(shape=(32, 1, 62, 62), dtype=float32)\n            \n            \n            Call arguments received by layer \"decoder_layer_16\" \"                 f\"(type DecoderLayer):\n               datum=tf.Tensor(shape=(32, 61, 64), dtype=float32)\n               encoder_result=tf.Tensor(shape=(32, 419, 64), dtype=float32)\n               is_training=True\n               look_mask=tf.Tensor(shape=(32, 1, 62, 62), dtype=float32)\n               pad_mask=tf.Tensor(shape=(32, 1, 1, 419), dtype=float32)\n        \n        \n        Call arguments received by layer \"decoder_4\" \"                 f\"(type Decoder):\n           datum=tf.Tensor(shape=(32, 61), dtype=int32)\n           decoded_output=tf.Tensor(shape=(32, 419, 64), dtype=float32)\n           is_training=True\n           look_mask=tf.Tensor(shape=(32, 1, 62, 62), dtype=float32)\n           pad_mask=tf.Tensor(shape=(32, 1, 1, 419), dtype=float32)\n    \n    \n    Call arguments received by layer \"transformer_4\" \"                 f\"(type Transformer):\n       input_data=tf.Tensor(shape=(32, 419), dtype=int32)\n       target_data=tf.Tensor(shape=(32, 61), dtype=int32)\n       pad_mask_enc=tf.Tensor(shape=(32, 1, 1, 419), dtype=float32)\n       pad_mask_dec=tf.Tensor(shape=(32, 1, 1, 419), dtype=float32)\n       look_mask_dec=tf.Tensor(shape=(32, 1, 62, 62), dtype=float32)\n       is_training=True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[96], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m loss_metric\u001b[38;5;241m.\u001b[39mreset_states()\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (batch, (inp, tar)) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):\n\u001b[1;32m---> 34\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# 55k samples\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# we display 3 batch results -- 0th, middle and last one (approx)\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# 55k / 64 ~ 858; 858 / 2 = 429\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m429\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file2jdopyo5.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train\u001b[1;34m(input_, target_)\u001b[0m\n\u001b[0;32m     10\u001b[0m masks \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(mask_maker), (ag__\u001b[38;5;241m.\u001b[39mld(input_), ag__\u001b[38;5;241m.\u001b[39mld(target_)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 12\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     Loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(loss), (ag__\u001b[38;5;241m.\u001b[39mld(target_actual), ag__\u001b[38;5;241m.\u001b[39mld(preds)[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     14\u001b[0m gradients \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(Loss), ag__\u001b[38;5;241m.\u001b[39mld(t)\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file6x4cvn8o.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, input_data, target_data, pad_mask_enc, pad_mask_dec, look_mask_dec, is_training)\u001b[0m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m     10\u001b[0m enc \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mencoder, (ag__\u001b[38;5;241m.\u001b[39mld(input_data), ag__\u001b[38;5;241m.\u001b[39mld(is_training), ag__\u001b[38;5;241m.\u001b[39mld(pad_mask_enc)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 11\u001b[0m dec_results \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_training\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlook_mask_dec\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpad_mask_dec\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m result \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39moutput_layer, (ag__\u001b[38;5;241m.\u001b[39mld(dec_results)[\u001b[38;5;241m0\u001b[39m],), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filemrimg9u6.py:35\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, datum, decoded_output, is_training, look_mask, pad_mask)\u001b[0m\n\u001b[0;32m     33\u001b[0m block1 \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblock1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     34\u001b[0m block2 \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblock2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfor_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDatum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miterate_names\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m(index, layer)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filemrimg9u6.py:28\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.loop_body\u001b[1;34m(itr)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mnonlocal\u001b[39;00m Datum\n\u001b[0;32m     27\u001b[0m (index, layer) \u001b[38;5;241m=\u001b[39m itr\n\u001b[1;32m---> 28\u001b[0m (Datum, block1, block2) \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDatum\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoded_output\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_training\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlook_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpad_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(attn_weights)[ag__\u001b[38;5;241m.\u001b[39mconverted_call(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder_layer\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_block1\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat, (ag__\u001b[38;5;241m.\u001b[39mld(index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)] \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(block1)\n\u001b[0;32m     30\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(attn_weights)[ag__\u001b[38;5;241m.\u001b[39mconverted_call(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder_layer\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_block2\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat, (ag__\u001b[38;5;241m.\u001b[39mld(index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)] \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(block2)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filen9wsstew.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, datum, encoder_result, is_training, look_mask, pad_mask)\u001b[0m\n\u001b[0;32m      8\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      9\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m---> 10\u001b[0m att1_res \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatum\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatum\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatum\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlook_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m att1_out \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(att1_res)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     12\u001b[0m att1_weights \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(att1_res)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filej9v05bqs.py:16\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[1;34m(self, q, v, k, mask)\u001b[0m\n\u001b[0;32m     14\u001b[0m split_V \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39msplit, (ag__\u001b[38;5;241m.\u001b[39mld(V), ag__\u001b[38;5;241m.\u001b[39mld(BATCH_SIZE)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     15\u001b[0m split_K \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39msplit, (ag__\u001b[38;5;241m.\u001b[39mld(K), ag__\u001b[38;5;241m.\u001b[39mld(BATCH_SIZE)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 16\u001b[0m attention \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43msdp_attention\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_Q\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_K\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_V\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m weights \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(attention)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     18\u001b[0m attention_results \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mtranspose, (ag__\u001b[38;5;241m.\u001b[39mld(attention)[\u001b[38;5;241m0\u001b[39m],), \u001b[38;5;28mdict\u001b[39m(perm\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m]), fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filecgcjf1ai.py:31\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__sdp_attention\u001b[1;34m(q, k, v, mask)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m scaled\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melse_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscaled\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m softmax \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax, (ag__\u001b[38;5;241m.\u001b[39mld(scaled),), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), fscope)\n\u001b[0;32m     33\u001b[0m result \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mmatmul, (ag__\u001b[38;5;241m.\u001b[39mld(softmax), ag__\u001b[38;5;241m.\u001b[39mld(v)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filecgcjf1ai.py:26\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__sdp_attention.<locals>.if_body\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mprint\u001b[39m)(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask:   \u001b[39m\u001b[38;5;124m'\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mld(mask))\n\u001b[0;32m     25\u001b[0m scaled \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(scaled)\n\u001b[1;32m---> 26\u001b[0m scaled \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m mask \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1000000000.0\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\emery\\AppData\\Local\\Temp\\ipykernel_13336\\3524479449.py\", line 12, in train  *\n        preds = t(\n    File \"C:\\Users\\emery\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_file6x4cvn8o.py\", line 11, in tf__call\n        dec_results = ag__.converted_call(ag__.ld(self).decoder, (ag__.ld(target_data), ag__.ld(enc), ag__.ld(is_training), ag__.ld(look_mask_dec), ag__.ld(pad_mask_dec)), None, fscope)\n    File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filemrimg9u6.py\", line 35, in tf__call\n        ag__.for_stmt(ag__.converted_call(ag__.ld(enumerate), (ag__.ld(self).decoder_layers,), None, fscope), None, loop_body, get_state, set_state, ('Datum',), {'iterate_names': '(index, layer)'})\n    File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filemrimg9u6.py\", line 28, in loop_body\n        (Datum, block1, block2) = ag__.converted_call(ag__.ld(layer), (ag__.ld(Datum), ag__.ld(decoded_output), ag__.ld(is_training), ag__.ld(look_mask), ag__.ld(pad_mask)), None, fscope)\n    File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filen9wsstew.py\", line 10, in tf__call\n        att1_res = ag__.converted_call(ag__.ld(self).att1, (ag__.ld(datum), ag__.ld(datum), ag__.ld(datum), ag__.ld(look_mask)), None, fscope)\n    File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filej9v05bqs.py\", line 16, in tf__call\n        attention = ag__.converted_call(ag__.ld(sdp_attention), (ag__.ld(split_Q), ag__.ld(split_K), ag__.ld(split_V), ag__.ld(mask)), None, fscope)\n    File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filecgcjf1ai.py\", line 31, in tf__sdp_attention\n        ag__.if_stmt(ag__.ld(mask) is not None, if_body, else_body, get_state, set_state, ('scaled',), 1)\n    File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filecgcjf1ai.py\", line 26, in if_body\n        scaled += mask * -1000000000.0\n\n    ValueError: Exception encountered when calling layer \"transformer_4\" \"                 f\"(type Transformer).\n    \n    in user code:\n    \n        File \"C:\\Users\\emery\\AppData\\Local\\Temp\\ipykernel_13336\\654210736.py\", line 18, in call  *\n            dec_results = self.decoder(target_data, enc, is_training, look_mask_dec, pad_mask_dec)\n        File \"C:\\Users\\emery\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filemrimg9u6.py\", line 35, in tf__call\n            ag__.for_stmt(ag__.converted_call(ag__.ld(enumerate), (ag__.ld(self).decoder_layers,), None, fscope), None, loop_body, get_state, set_state, ('Datum',), {'iterate_names': '(index, layer)'})\n        File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filemrimg9u6.py\", line 28, in loop_body\n            (Datum, block1, block2) = ag__.converted_call(ag__.ld(layer), (ag__.ld(Datum), ag__.ld(decoded_output), ag__.ld(is_training), ag__.ld(look_mask), ag__.ld(pad_mask)), None, fscope)\n        File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filen9wsstew.py\", line 10, in tf__call\n            att1_res = ag__.converted_call(ag__.ld(self).att1, (ag__.ld(datum), ag__.ld(datum), ag__.ld(datum), ag__.ld(look_mask)), None, fscope)\n        File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filej9v05bqs.py\", line 16, in tf__call\n            attention = ag__.converted_call(ag__.ld(sdp_attention), (ag__.ld(split_Q), ag__.ld(split_K), ag__.ld(split_V), ag__.ld(mask)), None, fscope)\n        File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filecgcjf1ai.py\", line 31, in tf__sdp_attention\n            ag__.if_stmt(ag__.ld(mask) is not None, if_body, else_body, get_state, set_state, ('scaled',), 1)\n        File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filecgcjf1ai.py\", line 26, in if_body\n            scaled += mask * -1000000000.0\n    \n        ValueError: Exception encountered when calling layer \"decoder_4\" \"                 f\"(type Decoder).\n        \n        in user code:\n        \n            File \"C:\\Users\\emery\\AppData\\Local\\Temp\\ipykernel_13336\\2448223447.py\", line 37, in call  *\n                Datum, block1, block2 = layer(Datum, decoded_output, is_training, look_mask, pad_mask)\n            File \"C:\\Users\\emery\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filen9wsstew.py\", line 10, in tf__call\n                att1_res = ag__.converted_call(ag__.ld(self).att1, (ag__.ld(datum), ag__.ld(datum), ag__.ld(datum), ag__.ld(look_mask)), None, fscope)\n            File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filej9v05bqs.py\", line 16, in tf__call\n                attention = ag__.converted_call(ag__.ld(sdp_attention), (ag__.ld(split_Q), ag__.ld(split_K), ag__.ld(split_V), ag__.ld(mask)), None, fscope)\n            File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filecgcjf1ai.py\", line 31, in tf__sdp_attention\n                ag__.if_stmt(ag__.ld(mask) is not None, if_body, else_body, get_state, set_state, ('scaled',), 1)\n            File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filecgcjf1ai.py\", line 26, in if_body\n                scaled += mask * -1000000000.0\n        \n            ValueError: Exception encountered when calling layer \"decoder_layer_16\" \"                 f\"(type DecoderLayer).\n            \n            in user code:\n            \n                File \"C:\\Users\\emery\\AppData\\Local\\Temp\\ipykernel_13336\\3294731785.py\", line 28, in call  *\n                    att1_res = self.att1(datum, datum, datum, look_mask)\n                File \"C:\\Users\\emery\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n                File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filej9v05bqs.py\", line 16, in tf__call\n                    attention = ag__.converted_call(ag__.ld(sdp_attention), (ag__.ld(split_Q), ag__.ld(split_K), ag__.ld(split_V), ag__.ld(mask)), None, fscope)\n                File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filecgcjf1ai.py\", line 31, in tf__sdp_attention\n                    ag__.if_stmt(ag__.ld(mask) is not None, if_body, else_body, get_state, set_state, ('scaled',), 1)\n                File \"C:\\Users\\emery\\AppData\\Local\\Temp\\__autograph_generated_filecgcjf1ai.py\", line 26, in if_body\n                    scaled += mask * -1000000000.0\n            \n                ValueError: Exception encountered when calling layer \"attention_layer_55\" \"                 f\"(type AttentionLayer).\n                \n                in user code:\n                \n                    File \"C:\\Users\\emery\\AppData\\Local\\Temp\\ipykernel_13336\\32935471.py\", line 33, in call  *\n                        attention = sdp_attention(split_Q, split_K, split_V, mask)\n                    File \"C:\\Users\\emery\\AppData\\Local\\Temp\\ipykernel_13336\\3890593852.py\", line 15, in sdp_attention  *\n                        scaled += (mask * -1e9)\n                \n                    ValueError: Dimensions must be equal, but are 61 and 62 for '{{node transformer_4/decoder_4/decoder_layer_16/attention_layer_55/add}} = AddV2[T=DT_FLOAT](transformer_4/decoder_4/decoder_layer_16/attention_layer_55/truediv, transformer_4/decoder_4/decoder_layer_16/attention_layer_55/mul)' with input shapes: [32,8,61,61], [32,1,62,62].\n                \n                \n                Call arguments received by layer \"attention_layer_55\" \"                 f\"(type AttentionLayer):\n                   q=tf.Tensor(shape=(32, 61, 64), dtype=float32)\n                   v=tf.Tensor(shape=(32, 61, 64), dtype=float32)\n                   k=tf.Tensor(shape=(32, 61, 64), dtype=float32)\n                   mask=tf.Tensor(shape=(32, 1, 62, 62), dtype=float32)\n            \n            \n            Call arguments received by layer \"decoder_layer_16\" \"                 f\"(type DecoderLayer):\n               datum=tf.Tensor(shape=(32, 61, 64), dtype=float32)\n               encoder_result=tf.Tensor(shape=(32, 419, 64), dtype=float32)\n               is_training=True\n               look_mask=tf.Tensor(shape=(32, 1, 62, 62), dtype=float32)\n               pad_mask=tf.Tensor(shape=(32, 1, 1, 419), dtype=float32)\n        \n        \n        Call arguments received by layer \"decoder_4\" \"                 f\"(type Decoder):\n           datum=tf.Tensor(shape=(32, 61), dtype=int32)\n           decoded_output=tf.Tensor(shape=(32, 419, 64), dtype=float32)\n           is_training=True\n           look_mask=tf.Tensor(shape=(32, 1, 62, 62), dtype=float32)\n           pad_mask=tf.Tensor(shape=(32, 1, 1, 419), dtype=float32)\n    \n    \n    Call arguments received by layer \"transformer_4\" \"                 f\"(type Transformer):\n       input_data=tf.Tensor(shape=(32, 419), dtype=int32)\n       target_data=tf.Tensor(shape=(32, 61), dtype=int32)\n       pad_mask_enc=tf.Tensor(shape=(32, 1, 1, 419), dtype=float32)\n       pad_mask_dec=tf.Tensor(shape=(32, 1, 1, 419), dtype=float32)\n       look_mask_dec=tf.Tensor(shape=(32, 1, 62, 62), dtype=float32)\n       is_training=True\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# the function annotation helps Tensorflow run this more efficiently\n",
    "@tf.function\n",
    "def train(input_, target_):\n",
    "    target_input  = target_[:, :-1]\n",
    "    target_actual = target_[:, 1:]\n",
    "    \n",
    "    masks = mask_maker(input_, target_)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        preds = t(\n",
    "            input_,\n",
    "            target_input,\n",
    "            masks[0], # pad_mask_enc\n",
    "            masks[1], # pad_mask_dec\n",
    "            masks[2], # look_mask_dec\n",
    "            True # is_training is true\n",
    "        )\n",
    "        \n",
    "        Loss = loss(target_actual, preds[0])\n",
    "    \n",
    "    gradients = tape.gradient(Loss, t.trainable_variables)\n",
    "    adam_optimizer.apply_gradients(zip(gradients, t.trainable_variables))\n",
    "\n",
    "    loss_metric(Loss)\n",
    "    \n",
    "# this part is directly copied, just to try to get it to work\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    loss_metric.reset_states()\n",
    "  \n",
    "    for (batch, (inp, tar)) in enumerate(dataset):\n",
    "        train(inp, tar)\n",
    "    \n",
    "        # 55k samples\n",
    "        # we display 3 batch results -- 0th, middle and last one (approx)\n",
    "        # 55k / 64 ~ 858; 858 / 2 = 429\n",
    "#         if batch % 429 == 0:\n",
    "#             print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, train_loss.result()))\n",
    "      \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
    "    \n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
