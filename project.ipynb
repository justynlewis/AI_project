{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Summaries\n",
    "## Justyn Lewis and Emery Jacobowitz\n",
    "\n",
    "---\n",
    "\n",
    "Reading is hard. It would be really nice if we could just plug texts into a computer program, which could succinctly explain what's going on. This is the summarization problem. How do we figure out what information a text is trying to convey, and what to report in our summary.\n",
    "\n",
    "There are two approaches to summarization.\n",
    "\n",
    "The first is **extractive** summarization, where we come up with some way of ranking how important sentences are, then cutting out everything except those sentences. This is pretty easy to do, and it retains the original linguistic structure, but it's not how humans summarize things.\n",
    "\n",
    "The \"natural\" approach is **abstractive** summarization. This is where we read through a text, and come up with a novel summary that captures the main ideas. This is the approach we will be implementing today.\n",
    "\n",
    "We start by importing a dataset. This is a set of Amazon reviews for various projects over the span of a few years. Each one comes with a sample summary, which we will use to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Getting the data\n",
    "# This is a dataset of 500k Amazon reviews\n",
    "# We only use 100k of these.\n",
    "data = pd.read_csv('data\\Reviews.csv', nrows=100000)\n",
    "data.drop_duplicates(subset=['Text'],inplace=True)  #dropping duplicates\n",
    "data.dropna(axis=0,inplace=True)   #dropping na\n",
    "text_data = data['Text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "2.10.0\n",
      "False\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(tf.keras.__version__)\n",
    "\n",
    "print(tf.test.is_built_with_cuda())\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prologue: Preparing the data\n",
    "\n",
    "The first step when dealing with raw text is to clean it up, so we will use NLTK's NLP utilities to do this. This step includes:\n",
    "- Case normalization\n",
    "- Lemmatizing\n",
    "    - i.e. replacing inflected words with their \"dictionary forms\"\n",
    "- Removing punctuation\n",
    "- etc.\n",
    "\n",
    "To accomplish this, we will use a **tokenizer** from Keras. The goal is to train the tokenizer on our dataset, to create a \"vocabulary\" which accurately represents the kind of things we are going to summarize. The tokenizer also cleans the data, as above.\n",
    "\n",
    "Before the tokenizer runs, we first need to append all of the summaries with tags representing their starting and ending points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_START_ Good Quality Dog Food _END_',\n",
       " '_START_ Not as Advertised _END_',\n",
       " '_START_ \"Delight\" says it all _END_']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_data = data['Summary']\n",
    "summary_data = ['_START_ ' + summary + ' _END_' for summary in summary_data]\n",
    "\n",
    "summary_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# AKA out-of-vocabulary token\n",
    "# For unknown words that aren't relevant to tokenization\n",
    "oov = '_oov_'\n",
    "\n",
    "# initialize the tokenizer\n",
    "text_tokenizer = Tokenizer(oov_token=oov)\n",
    "\n",
    "# fit it on our dataset\n",
    "text_tokenizer.fit_on_texts(text_data)\n",
    "\n",
    "# split the text up into sequences\n",
    "input_seqs = text_tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "# repeat for the summaries\n",
    "summary_tokenizer = Tokenizer(oov_token=oov)\n",
    "summary_tokenizer.fit_on_texts(summary_data)\n",
    "target_seqs = text_tokenizer.texts_to_sequences(summary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vocabulary size:  60712\n",
      "Target vocabulary size:  15822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[51], [56], [537], [11973], []]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Input vocabulary size: ', len(text_tokenizer.word_index))\n",
    "print('Target vocabulary size: ', len(summary_tokenizer.word_index))\n",
    "text_tokenizer.texts_to_sequences(['We', 'love', 'Artificial', 'Intelligence', '!'])\n",
    "# The output is a measure of how frequent these terms are in our dataset.\n",
    "# Notice that \"!\" is removed, because the tokenizer strips out punctuation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last bit of preprocessing is to pad and truncate our sequences, since we are going to want to always feed in inputs of the same length to our model. This is why we needed to label the starts and ends of the summaries. The maximum length after preprocessing will just be the average item length plus a buffer for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input maxlen:   468\n",
      "Target maxlen:  62\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from math import floor\n",
    "import numpy\n",
    "\n",
    "input_max_len = floor(numpy.average([len(item) for item in text_data])) + 25\n",
    "target_max_len = floor(numpy.average([len(item) for item in summary_data])) + 25\n",
    "\n",
    "print(\"Input maxlen:  \", input_max_len)\n",
    "print(\"Target maxlen: \", target_max_len)\n",
    "\n",
    "# the 'post' options mean we pad/truncate at the end of the sequence, not before it.\n",
    "input_seqs = pad_sequences(input_seqs, maxlen=input_max_len, padding='post', truncating='post')\n",
    "target_seqs = pad_sequences(target_seqs, maxlen=target_max_len, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A final adjustment is to set the buffer and batch sizes for our model, producing a randomized Tensorflow Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(None, 468), dtype=tf.int32, name=None), TensorSpec(shape=(None, 62), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER = 20000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "input_seqs = tf.cast(input_seqs, dtype=tf.int32)\n",
    "target_seqs = tf.cast(target_seqs, dtype=tf.int32)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_seqs, target_seqs)).shuffle(BUFFER).batch(BATCH_SIZE)\n",
    "dataset"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer Architecture, pt. I\n",
    "---\n",
    "### Data Utilities\n",
    "The thing about self-attention in transformers is we could end up losing ordinal information about our data. To solve this problem, we use \"positional encodings\", where we embed ordinal information within our word data. We want to implement these formulae:\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Math is hard.\n",
    "# This section is closely adapted from https://medium.com/swlh/abstractive-text-summarization-using-transformers-3e774cc42453\n",
    "\n",
    "# calculate PE\n",
    "def positional_encoding(pos, i, d_model):\n",
    "    angle_rads = get_angles(\n",
    "        np.arange(position)[:, np.newaxis],\n",
    "        np.arange(d_model)[np.newaxis, :],\n",
    "        d_model\n",
    "    )\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# generate the internal argument to the trig function\n",
    "def get_angles(position, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return position * angle_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need two small masks.\n",
    "\n",
    "One is the padding mask, which ignores the padded portions of our sequences.\n",
    "\n",
    "The other is the lookahead mask, which ignores words that come after a given word. This is important because we want to make predictions based on the words we have seen already, since language is linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[0. 0. 0. ... 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 1. 1. 1.]]]], shape=(3, 1, 1, 468), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[0. 0. 0. ... 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 1. 1. 1.]]]], shape=(4, 1, 1, 468), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[[0. 0. 0. ... 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 1. 1. 1.]]]\n",
      "\n",
      "\n",
      " [[[0. 0. 0. ... 1. 1. 1.]]]], shape=(5, 1, 1, 468), dtype=float32)\n",
      "==================================================\n",
      "tf.Tensor(\n",
      "[[0. 1. 1. 1. 1.]\n",
      " [0. 0. 1. 1. 1.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]], shape=(5, 5), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. 1. 1. 1.]\n",
      " [0. 0. 1. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]], shape=(4, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. 1. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 0.]], shape=(3, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. 1.]\n",
      " [0. 0.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def padding_mask(seq):\n",
    "    # we padded with 0s\n",
    "    remove_padding = tf.math.equal(seq, 0)\n",
    "    new_seq = tf.cast(remove_padding, tf.float32)\n",
    "    # add two new axes\n",
    "    return new_seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "# Once again, I don't really understand how this mask works. It's from the same source above.\n",
    "def lookahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "print(padding_mask(input_seqs[:3]))\n",
    "print(padding_mask(input_seqs[:4]))\n",
    "print(padding_mask(input_seqs[:5]))\n",
    "print('==================================================')\n",
    "print(lookahead_mask(5))\n",
    "print(lookahead_mask(4))\n",
    "print(lookahead_mask(3))\n",
    "print(lookahead_mask(2))"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer Architecture, pt. 2\n",
    "---\n",
    "### Paying Self-Attention\n",
    "\n",
    "At this point, we can start assembling the transformer. The first step is to implement a scaled dot product function, which is the key to our self-attention layer. This includes a few components: \n",
    "- *Matrix multiplication* (i.e. dot product) of our query and our key\n",
    "- *Scaling*\n",
    "    - We need to scale down our inputs in order to prevent them from being too large, so we divide by $\\sqrt{d_k}$, a scaling factor proportional to the size of our input.\n",
    "- An optional *mask*\n",
    "    - This is where we will use our padding mask from before\n",
    "- *Softmax*\n",
    "    - Transforming the value to fit in a \\[0, 1\\] probability distribution.\n",
    "- Another step of matrix multiplication of the result of the above with our value.\n",
    "\n",
    "Thus, the scaled dot product attention is equivalent to ![image.png](attachment:image.png) although the affect of the mask is not counted here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in a query, a key, a value, and an optional mask.\n",
    "# returns a tuple of (the output, the attention weights)\n",
    "def sdp_attention(q, k, v, mask=None):\n",
    "    # Step 1: QK^T, AKA matrix multiplication\n",
    "    qk = tf.matmul(q, k, transpose_b=True)\n",
    "    \n",
    "    #Scaling\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled = qk / dk\n",
    "    \n",
    "    # apply the mask, only if it is not the default None option.\n",
    "    if mask is not None:\n",
    "        scaled += (mask * -1e9)\n",
    "    \n",
    "    # softmax\n",
    "    softmax = tf.nn.softmax(scaled)\n",
    "    \n",
    "    # finally, multiply by v\n",
    "    result = tf.matmul(softmax, v)\n",
    "    return result, softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to actually make this into a layer in our Transformer architecture. We do this by implementing a subclass of the Layer class from Keras. The important method that we implement is call(), which applies the logic of the layer. Keras documentation also suggests implementing a few other methods, but we don't think those are necessary for our project, so we are going to tactically ignore them.\n",
    "\n",
    "We make it \"multi-headed\", which means that we can split up the calculations to make them more efficient. If we want to split into *h* heads, then the sizes of the query, key, and value (AKA the *depth*) will be equal to *d_model* / *h*. Therefore, the former must be evenly divisible by the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.AttentionLayer at 0x1da0967a920>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, heads):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.heads = heads\n",
    "        \n",
    "        # d_model needs to be divisible by the number of heads\n",
    "        if not d_model % heads == 0:\n",
    "            raise RuntimeError('d_model (' + str(d_model) + ') must be divisible by heads (' + str(heads) + ').')\n",
    "        \n",
    "        self.depth = d_model / heads\n",
    "        \n",
    "        # make Keras densely-connected NN layers for the data we eventually have\n",
    "        self.q_layer = tf.keras.layers.Dense(d_model)\n",
    "        self.k_layer = tf.keras.layers.Dense(d_model)\n",
    "        self.v_layer = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        # dense layer that will eventually store the output\n",
    "        self.out_layer = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    def call(self, q, v, k, mask):       \n",
    "        # start applying all those layers we made\n",
    "        Q = self.q_layer(q)\n",
    "        V = self.q_layer(v)\n",
    "        K = self.q_layer(k)\n",
    "        \n",
    "        # split the heads apart, so that we are multi-headed\n",
    "        split_Q = self.split(Q, batch)\n",
    "        split_V = self.split(V, batch)\n",
    "        split_K = self.split(K, batch)\n",
    "    \n",
    "        # apply the attention\n",
    "        attention = sdp_attention(q, k, v, mask)\n",
    "        weights = attention[1]\n",
    "        attention_results = tf.transpose(attention[0], perm=[0,2,1,3])\n",
    "        \n",
    "        # concatenate attention back to one vector\n",
    "        attention_results = tf.reshape(attention_results, (tf.shape(q)[0], -1, self.d_model))\n",
    "        output = self.out_layer(attention_results)\n",
    "        \n",
    "        return output, weights\n",
    "        \n",
    "    \n",
    "    # helper method for splitting the heads\n",
    "    def split(self, layer, batch):\n",
    "        Layer = tf.reshape(layer, (batch, -1, self.heads, self.depth))\n",
    "        Layer = tf.transpose(Layer, perm=[0,2,1,3])\n",
    "        return Layer\n",
    "        \n",
    "AttentionLayer(6, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feeding it Forward\n",
    "\n",
    "The last basic unit of a transformer that we have to implement is a Feed-Forward Network (FFN) layer. This is pretty simple, since we just need to sequentially go from one layer to another, and Keras has this functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffn(d_model, d_ffn):\n",
    "    l1 = tf.keras.layers.Dense(d_ffn)\n",
    "    l2 = tf.keras.layers.Dense(d_model)\n",
    "    return tf.keras.Sequential([l1, l2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transformer Architecture, pt. 3\n",
    "---\n",
    "### Building the Transformer \n",
    "\n",
    "So far, we've made the individual building blocks of the Transformer, which process data at each step. Now, we can build the main components: the Encoder and the Decoder. These are comprised of individual layers, each of which contain:\n",
    "- An attention layer\n",
    "- A FFN layer\n",
    "- A normalization layer\n",
    "- A dropout layer\n",
    "    - When training, randomly sets some inputs to 0, and distributes that value to the other inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, heads, d_ffn):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        # the MH Attention layer\n",
    "        self.att = AttentionLayer(d_model, heads)\n",
    "        \n",
    "        # the FFN  layer\n",
    "        self.ffn = ffn(d_model, d_ffn)\n",
    "        \n",
    "        # Normie layers\n",
    "        # we use a smaller epsilon value than standard\n",
    "        norm1 = tf.keras.layers.LayerNormalization(0.000001)\n",
    "        norm2 = tf.keras.layers.LayerNormalization(0.000001)\n",
    "        \n",
    "        # Dropout layers\n",
    "        # We apply dropout 10% of the time.\n",
    "        drop1 = tf.keras.layers.Dropout(0.1)\n",
    "        drop2 = tf.keras.layers.Dropout(0.1)\n",
    "        \n",
    "    def call(self, datum, is_training, mask):\n",
    "        # Sorry for the weird formatting.\n",
    "        # I think this is the best way to present the process\n",
    "        \n",
    "        # <...>   <---   normalize <--  dropout    <--   attention <-- input\n",
    "        attention = self.norm1( datum + self.drop1( self.att(datum, datum, datum, mask), training=is_training))\n",
    "        \n",
    "        #           normalize     <--       dropout <-- ffn <-- <...>\n",
    "        feed = self.norm2( attention + self.drop2( self.ffn(attention), training=is_training))\n",
    "        \n",
    "        return feed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Decoder layer is pretty much the same as an Encoder layer, except that it has that extra attention layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, heads, d_ffn):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        # the MH Attention layers\n",
    "        self.att1 = AttentionLayer(d_model, heads)\n",
    "        self.att2 = AttentionLayer(d_model, heads)\n",
    "        \n",
    "        # the FFN  layer\n",
    "        self.ffn = ffn(d_model, d_ffn)\n",
    "        \n",
    "        # Normie layers\n",
    "        # we use a smaller epsilon value than standard\n",
    "        norm1 = tf.keras.layers.LayerNormalization(0.000001)\n",
    "        norm2 = tf.keras.layers.LayerNormalization(0.000001)\n",
    "        norm3 = tf.keras.layers.LayerNormalization(0.000001)\n",
    "        \n",
    "        # Dropout layers\n",
    "        # We apply dropout 10% of the time.\n",
    "        drop1 = tf.keras.layers.Dropout(0.1)\n",
    "        drop2 = tf.keras.layers.Dropout(0.1)\n",
    "        drop3 = tf.keras.layers.Dropout(0.1)\n",
    "        \n",
    "    # The decoder actually needs to use the masks we defined earlier\n",
    "    def call(self, datum, encoder_result, is_training, look_mask, pad_mask):\n",
    "        \n",
    "        # The first attention layer\n",
    "        att1_res = self.att1(datum, datum, datum, look_mask)\n",
    "        att1_out = att1_res[0]\n",
    "        att1_weights = att1_res[1]\n",
    "        att1_out = self.norm1(datum + (self.drop1(att1_out, training=is_training)))\n",
    "        \n",
    "        # This extra attention layer takes in the input from the encoder as a query\n",
    "        # and combines it with the first layer's output, using. \n",
    "        att2_res = self.att2(out1, encoder_result, encoder_result, pad_mask)\n",
    "        att2_out = att2_res[0]\n",
    "        att2_weights = att1_res[1]\n",
    "        att2_out = self.norm2(att1_out + (self.drop2(att2_out, training=is_training)))\n",
    "        \n",
    "        # Finally, the ffn layer\n",
    "        ffn_out = self.norm3(att2_out + self.drop3(self.ffn(att2_out), training=is_training))\n",
    "        \n",
    "        return ffn_out, att1_weights, att2_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembling the components\n",
    "\n",
    "Now that we have our Encoder/Decoder layers, we can actually put them together to form the Encoder and Decoder themselves.\n",
    "\n",
    "These are basically just bigger Keras Layers that have multiple smaller layers inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, layers, d_model, heads, d_ffn, vocab_size, max_pos):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.layers = layers\n",
    "\n",
    "        # creates an embedding layer with the size of the vocabulary\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, self.d_model)\n",
    "        \n",
    "        self.pos_enc = positional_encoding(max_pos, self.d_model)\n",
    "\n",
    "        self.encoder_layers = []\n",
    "        for i in range(layers):\n",
    "            l = EncoderLayer(d_model, heads, d_ffn)\n",
    "            self.encoder_layers.append(l)\n",
    "\n",
    "        self.drop = tf.keras.layers.Dropout(0.1)\n",
    "    \n",
    "    def call(self, datum, is_training, mask):\n",
    "        # get the length of the input\n",
    "        length = tf.shape(datum)[1]\n",
    "        \n",
    "        # get the input embedding\n",
    "        Datum = self.embedding(datum)\n",
    "        Datum = Datum * tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        \n",
    "        # apply the positional encoding\n",
    "        Datum += self.pos_enc[:, :length, :]\n",
    "        \n",
    "        # apply dropout\n",
    "        Datum = self.drop(Datum, training=is_training)\n",
    "        \n",
    "        # apply all of the Encoder layers in order\n",
    "        for layer in self.encoder_layers:\n",
    "            Datum = layer(Datum, is_training, mask)\n",
    "        \n",
    "        return Datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\tmodified:   project.ipynb\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    }
   ],
   "source": [
    "!git "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
